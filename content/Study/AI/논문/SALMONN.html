<!DOCTYPE html>
<html lang="en" dir="ltr"><head><title>SALMONN</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="JEngine"/><meta property="og:title" content="SALMONN"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="SALMONN"/><meta name="twitter:description" content="모델의 목표 LLM에 일반적인 청각 능력을 부여한 멀티모달 기존에는 특정 작업에 특화된 모델들이 사용됬지만 좀 더 범용적인 모델을 위해 개발 이 논문의 도전 과제 서로 다른 오디오 작업을 하나의 모델에서 통합적으로 처리 ex) 음성 인식, 음악 설명, 감정 인식 등 새로운 오디오 작업이나 데이터에 대한 적응력 향상 처음보는 데이터를 처리할 수 있도록 다중 모달(텍스트 + 오디오) 데이터를 동시에 이해할 수 있는 능력 모델의 구조 두 가지 주요 컴포넌트로 구성 Text LLM 사전 학습된 텍스트 기반 LLM(GPT 계열) 사용 Aud..."/><meta property="og:description" content="모델의 목표 LLM에 일반적인 청각 능력을 부여한 멀티모달 기존에는 특정 작업에 특화된 모델들이 사용됬지만 좀 더 범용적인 모델을 위해 개발 이 논문의 도전 과제 서로 다른 오디오 작업을 하나의 모델에서 통합적으로 처리 ex) 음성 인식, 음악 설명, 감정 인식 등 새로운 오디오 작업이나 데이터에 대한 적응력 향상 처음보는 데이터를 처리할 수 있도록 다중 모달(텍스트 + 오디오) 데이터를 동시에 이해할 수 있는 능력 모델의 구조 두 가지 주요 컴포넌트로 구성 Text LLM 사전 학습된 텍스트 기반 LLM(GPT 계열) 사용 Aud..."/><meta property="og:image:alt" content="모델의 목표 LLM에 일반적인 청각 능력을 부여한 멀티모달 기존에는 특정 작업에 특화된 모델들이 사용됬지만 좀 더 범용적인 모델을 위해 개발 이 논문의 도전 과제 서로 다른 오디오 작업을 하나의 모델에서 통합적으로 처리 ex) 음성 인식, 음악 설명, 감정 인식 등 새로운 오디오 작업이나 데이터에 대한 적응력 향상 처음보는 데이터를 처리할 수 있도록 다중 모달(텍스트 + 오디오) 데이터를 동시에 이해할 수 있는 능력 모델의 구조 두 가지 주요 컴포넌트로 구성 Text LLM 사전 학습된 텍스트 기반 LLM(GPT 계열) 사용 Aud..."/><meta property="twitter:domain" content="jin-sukkim.github.io/Blog"/><meta property="og:url" content="https://jin-sukkim.github.io/Blog/content/Study/AI/논문/SALMONN"/><meta property="twitter:url" content="https://jin-sukkim.github.io/Blog/content/Study/AI/논문/SALMONN"/><link rel="icon" href="../../../../static/icon.png"/><meta name="description" content="모델의 목표 LLM에 일반적인 청각 능력을 부여한 멀티모달 기존에는 특정 작업에 특화된 모델들이 사용됬지만 좀 더 범용적인 모델을 위해 개발 이 논문의 도전 과제 서로 다른 오디오 작업을 하나의 모델에서 통합적으로 처리 ex) 음성 인식, 음악 설명, 감정 인식 등 새로운 오디오 작업이나 데이터에 대한 적응력 향상 처음보는 데이터를 처리할 수 있도록 다중 모달(텍스트 + 오디오) 데이터를 동시에 이해할 수 있는 능력 모델의 구조 두 가지 주요 컴포넌트로 구성 Text LLM 사전 학습된 텍스트 기반 LLM(GPT 계열) 사용 Aud..."/><meta name="generator" content="Quartz"/><link href="../../../../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://jin-sukkim.github.io/Blog/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://jin-sukkim.github.io/Blog/content/Study/AI/논문/SALMONN-og-image.webp"/><meta property="og:image:url" content="https://jin-sukkim.github.io/Blog/content/Study/AI/논문/SALMONN-og-image.webp"/><meta name="twitter:image" content="https://jin-sukkim.github.io/Blog/content/Study/AI/논문/SALMONN-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="content/Study/AI/논문/SALMONN"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../../../..">JEngine</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg><p>Search</p></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>(node.name===\&quot;content\&quot;&amp;&amp;node.children&amp;&amp;(node.displayName=\&quot;\&quot;,node.collapsed=!1),node)&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="explorer-24" class="explorer-content" aria-expanded="false" role="group"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../content/">JEngine Blog</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../content/Study/">Study</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../content/Study/AI/">AI</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../../../content/Study/AI/논문/">논문</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>SALMONN</a></div></nav><h1 class="article-title">SALMONN</h1><p show-comma="true" class="content-meta"><time datetime="2025-10-24T13:42:27.353Z">Oct 24, 2025</time><span>14 min read</span></p><ul class="tags"><li><a href="../../../../tags/AI" class="internal tag-link">AI</a></li></ul></div></div><article class="popover-hint"><h2 id="모델의-목표">모델의 목표<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#모델의-목표" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>LLM에 일반적인 청각 능력을 부여한 멀티모달
<ul>
<li>기존에는 특정 작업에 특화된 모델들이 사용됬지만 좀 더 범용적인 모델을 위해 개발</li>
</ul>
</li>
<li>이 논문의 도전 과제
<ol>
<li>서로 다른 오디오 작업을 하나의 모델에서 통합적으로 처리
<ul>
<li>ex) 음성 인식, 음악 설명, 감정 인식 등</li>
</ul>
</li>
<li>새로운 오디오 작업이나 데이터에 대한 적응력 향상
<ul>
<li>처음보는 데이터를 처리할 수 있도록</li>
</ul>
</li>
<li>다중 모달(텍스트 + 오디오) 데이터를 동시에 이해할 수 있는 능력</li>
</ol>
</li>
</ul>
<h2 id="모델의-구조">모델의 구조<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#모델의-구조" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>두 가지 주요 컴포넌트로 구성</p>
<ol>
<li>Text LLM
<ul>
<li>사전 학습된 텍스트 기반 LLM(GPT 계열) 사용</li>
<li>Audio Encoder에서 추출된 정보를 바탕으로 텍스트 형식의 답변이나 설명을 생성</li>
</ul>
</li>
<li>Audio Encoder
<ul>
<li>음성 및 오디오 데이터를 Vector 표현으로 변환하는 모듈 (다양한 청각 정보를 처리)
<ul>
<li>입력 Audio를 먼저 Token화 진행</li>
</ul>
</li>
<li>오디오 신호에서 중요한 정보를 추출해 LLM에 입력</li>
<li>기존의 Whisper 모델과 같은 강력한 음성 Encoder 활용 가능</li>
<li>Speech audio와 Non-Speed Audio 둘 모두 좋은 성능을 내기 위해 dual encoder structure를 사용
<ul>
<li>Whisper speech 모델(Speech)과 BEATs audio(Non-Speech) encoder 사용</li>
<li>출력 특성이 동기화되고 결합</li>
</ul>
</li>
<li>두 Encoders가 동일한 출력 frame rate인 50Hz을 가지므로 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">n</span><span class="mord mathnormal">co</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">hi</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">er</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">n</span><span class="mord mathnormal">co</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)))</span></span></span></span>
<ul>
<li>X = 가변 길이 audio 입력 sequence, Z = T 프레임을 가진 Encoder 출력 Sequence</li>
</ul>
</li>
</ul>
</li>
<li>멀티모달 연결 (Multi-Modal Fusion)
<ul>
<li>오디오와 텍스트 데이터를 통합해 하나의 일관된 표현을 생성</li>
<li>이 과정에서 오디오 정보를 테스트 기반의 이해로 변환</li>
</ul>
</li>
</ol>
<ul>
<li>오디오 데이터는 이 Encoder를 통해 특징이 추출되며 LLM은 이를 바탕으로 의미를 이해하고 응답
<img src="../../../../SALMONN-model-architecture.png" width="700" height="auto" alt/></li>
</ul>
<h3 id="window-level-q-former">Window-level Q-Former<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#window-level-q-former" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>window-level query Transformer</li>
<li>Q-Former는 보통 LLM과 다른 입력(image 등)을 연결해주는 새로운 Transformer 방식
<ul>
<li>간단한 Transformer 구조</li>
</ul>
</li>
<li>즉, 서로 다른 데이터를 align 해주는 단계</li>
<li>오디오 데이터를 처리할 때 인코더가 생성하는 가변 길이 출력 시퀀스를 여러 개의 증강된 Audio Token으로 바꿔 Vicuna LLM의 입력으로 사용할 수 있게 해주는 연결 모듈로 사용</li>
<li>출력으로 text instruction promp과 Audio Encoder 출력을 통합해 LoRA Adapter와 LLM에 입력으로 사용
<img src="../../../../Q-Former-example.png" width="700" height="auto" alt/></li>
</ul>
<h4 id="핵심-개념">핵심 개념<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#핵심-개념" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>기본 Q-Former</p>
<ul>
<li>보통 고정된 길이의 Input Token만 사용 (ex: Image)
<ul>
<li>Audio 데이터를 작은 조각(Window)로 나누고 각 조각을 이미지처럼 처리해 token으로 변환해 가변적인 오디오를 처리</li>
</ul>
</li>
<li>Image Encoder의 출력을 고정된 개수의 텍스트 입력 토큰으로 변환하기 위해 자주 사용됨</li>
<li>하지만 오디오 입력은 길이가 가변적이기에 수정이 필요</li>
<li>구조 :
<ul>
<li>이미지 입력 <code>Z1</code>을 고정된 개수 <code>N</code>의 학습 가능한 쿼리 <code>Q</code>를 사용해 <code>H1</code>이라는 텍스트 토큰으로 변환</li>
<li>Q-Former Block은 Transformer decoder block과 비슷하지만 두 가지 차이점이 존재
<ol>
<li>첫 블록에서 고정된 학습 가능한 쿼리 <code>Q</code>를 사용</li>
<li>self-attention 레이어에서 causal mask 제거</li>
</ol>
</li>
</ul>
</li>
<li>이 구조를 통해 쿼리 Q가 서로 먼저 침조한 후, 입력 이미지 <code>Z1</code>과 <a href="../../../../Cross-Attention" class="internal" data-slug="Cross-Attention">Cross-Attention</a>으로 상호작용</li>
</ul>
<p>오디오 입력 처리</p>
<ul>
<li>오디오 입력 <code>Z</code>는 길이가 가변적이므로 직접 텍스트 토큰으로 변환하기가 어려움</li>
<li>이를 해결하기 위해 입력 <code>Z</code>를 길이 <strong><code>L</code>의 윈도우로 나누고, 마지막 윈도우는 0으로 Padding</strong></li>
<li>이렇게 나뉜 window의 Encoder 출력 frame을 이미지 처럼 취급해 Window 수준에서 Q-Former을 사용해 토큰 <code>H</code>로 변환</li>
<li>최종 텍스트 Token Sequence인 <code>H</code>는 총 <code>[T/L] x N</code>개의 텍스트 토큰을 포함
<ul>
<li>T = Audio 전체 길이</li>
</ul>
</li>
</ul>
<p>Q-Former을 사용해 dual Audio Encoder인 Whisper speech encoder와 BEATs audio encoder의 출력 Token을 연결해 결합
이는 증강된 오디오 토큰으로 LLM의 입력 공간과 align됨
(두 Audio Encoder는 Freeze하고 Q-Former만 학습, 즉 쿼리 Q를 학습시킴)</p>
<h3 id="lora">LoRA<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#lora" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>Vicuna에 적용되 Vicuna의 입력 공간과 출력 공간을 일치시키는 Cross-Modal Adapter로 사용되며, 성능을 향상시킴
<ul>
<li>Cross-Modal : 서로 다른 형태의 데이터를 다룰 수 있게 만든다는 의미</li>
</ul>
</li>
<li>즉, <a href="../../../../LoRA" class="internal" data-slug="LoRA">LoRA</a>로 LLM 모델의 입력과 Q-Former의 출력이 서로 잘 맞도록 조정하는 역할을 수행</li>
<li>훈련을 최적화하는 역할도 수행
<ul>
<li>LLM을 Freeze하고 LoRA Adapter만 훈련</li>
<li>self-attention layer에서 Querry와 Value 가중치 행렬에 적용</li>
</ul>
</li>
</ul>
<h2 id="훈련-및-학습-방법">훈련 및 학습 방법<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#훈련-및-학습-방법" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>모델은 다양한 오디오 Task에서 학습되었음</p>
<ul>
<li>음성 인식(ASR) : 음성을 텍스트로 변환</li>
<li>음성 번역 : 다른 언어로 음성 데이터를 번역</li>
<li>질의응답: 오디오 기반 질의에 대한 응답</li>
<li>음악 및 오디오 캡셔닝 : 음악이나 일반 오디오에 대한 설명 생성</li>
<li>화자 검증 및 감정 인식 : 특정 화자의 확인 및 감정 상태 감지</li>
</ul>
<p>훈련되지 않은 작업에 대한 일반화 능력을 높이기 위해 <a href="../../../../Few-Shot-Activation-Tuning" class="internal alias" data-slug="Few-Shot-Activation-Tuning">Few-Shot Activation Tuning</a> 기법을 사용</p>
<ul>
<li>이는 모델이 적은 데이터로도 새로운 작업에 적응하도록 도와줌</li>
</ul>
<h3 id="학습-방법">학습 방법<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#학습-방법" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>3가지 단계로 나눠서 학습</p>
<h4 id="1-pre-training-state">1. Pre-training State<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#1-pre-training-state" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<ul>
<li>사전 학습된 LLM &amp; Audio Encoders와 랜덤값으로 초기화된 Q-Former(Connection Module) &amp; LoRA(Adaptor)의 파라미터들 사이의 차이를 줄이기 위한 학습 단계
<ul>
<li>많은 양의 오디오 데이터를 사용해 window-level Q-Former와 LoRA를 사전 학습 진행</li>
</ul>
</li>
<li>이 작업엔 speech와 non-speech 오디오 내용에 대한 주요 정보를 포함하고 있으며, 복잡한 추론과 이해를 요구하지 않기 때문에 audio와 texture간의 alignment(연결, 매핑) 품질을 높이는데 도움이 됨</li>
</ul>
<p>방대한 양의 오디오-텍스트 훈련 세트를 사용해서 훈련을 진행</p>
<h4 id="2-instruction-tuning-stage">2. Instruction Tuning Stage<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#2-instruction-tuning-stage" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p><img src="../../../../audio-training-data-used-in-the-cross-modal-instruction-tuning-stage.png" width="700" height="auto" alt/></p>
<ul>
<li>Audio-Text Instruction Tuning
<ul>
<li>NLP나 Vision-Language 모델에서처럼 Audio와 텍스트를 함께 학습하는 방식</li>
<li>이 단계에서 음성 인식, Audio 이벤트 감지, Music Task 같은 다양한 작업을 학습</li>
</ul>
</li>
<li>Audio Data와 연결된 Text를 기반으로 모델이 이해할 수 있는 명령어를 생성해 학습에 사용</li>
</ul>
<p>여러 Task를 포함하며 질문이 Text Caption Labels을 기반으로 GPT를 사용해 생성되며, 모델은 일반 오디오 입력과 질문이 있는 Text prompt를 기반으로 답변을 제공해야 됨</p>
<ul>
<li>위의 표가 이 단계에서 사용되는 데이터</li>
<li>Task : ASR(자동 음성 인식), 자동 음성 번역(AST), AAC(오디오 설명 생성), 전화 인식(PR), 감정 인식(ER), 음악 캡션 생성(MC), 중첩 음성 인식(OSR), 화자 인증(SV), 성별 인식(GR), 음성 질문 응답(SQA), 오디오 질문 응답(AQA), 음악 질문 응답(MQA) 등</li>
</ul>
<h5 id="task-over-fitting-문제">Task Over-fitting 문제<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#task-over-fitting-문제" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h5>
<ul>
<li>위의 두 단계만으로 경쟁력있는 결과를 생성할 수 있지만 훈련되지 않은 작업을 수행하는 능력이 거의 없거나 이상한 응답을 생성하는 경우가 발생
<ul>
<li>이 현상을 task over-fitting이라 부름</li>
</ul>
</li>
<li>task over-fitting의 두 가지 이유
<ol>
<li>LLM의 훈련에 사용되는 text-only data와 비교해서 cross-modal instruction tuning에 사용된 prompts가 너무 간단해 생성된 응답이 복잡하고 다양하지 않음</li>
<li>Instruction Tuning에 포함된 일부 작업(특히 음성 인식, Audio Captioning)이 다른 작업들(speech &amp; audio 질의응답) 보다 결정론적인 출력을 가짐
<ul>
<li>결정론적인 출력(Deterministic Output) : 동일한 입력이 주어지면 항상 동일한 출력값이 나옴</li>
</ul>
</li>
</ol>
<ul>
<li>이 두 가지 이유가 결합되어서 일반화 성능이 떨어지게 되고 훈련되지 않은 Cross-modal task에 방해됨</li>
<li>Test시 새로운 명령 prompt <code>I</code>에 대해 주어진 테스트 입력 <code>X</code>의 response text sequence <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9233em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9233em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span><span class="svg-align" style="top:-3.6833em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span></span></span></span></span></span></span>는 다음과 같이 생성되고 이를 훈련동안 maximise해야됨 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9233em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9233em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span><span class="svg-align" style="top:-3.6833em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none"><path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2389em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∧</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span></li>
<li>모델이 훈련에서 제한된 텍스트 response만 보았기 때문에, 이 확률은 일부 작업에 편향됨
<ul>
<li>자동 음성 인식(ASR) 및 자동 오디오 캡셔닝(AAC) 작업 등</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-activation-tuning-stage">3. Activation Tuning Stage:<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#3-activation-tuning-stage" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<ul>
<li>task over-fitting을 해결하기 위한 효과적인 접근 방식</li>
<li>모델을 더 길고 다양한 응답을 가진 task에서 fine-tuning하는 방법
<ul>
<li>task : Audio-information-based question answering, storytelling 작업 등</li>
</ul>
</li>
<li>이런 task에 대한 훈련 데이터 pair는 audio-text 쌍으로 사람이나 LLM에 의해 생성합
<ul>
<li>훈련 데이터 pair : test-speech recognition, text-audio and music caption data</li>
</ul>
</li>
<li>SALMONN에선 단순히 LoRA adaptor의 scaling factor를 줄여서 zero-shot instructions에 대해 더 길고 다양한 응답을 생성할 수 있게함
<ul>
<li>이는 훈련에서 Q-Former와 LoRA로만 업데이트되기에 확률을 regulaization하는 대안적인 방법</li>
<li>LoRA adaptor의 scaling factor를 줄이면 질문 응답 및 스토리텔링 능력을 활성화하고 길고 다양한 응답을 생성 가능하지만 훈련된 task에 대한 결과의 품질은 저하됨</li>
</ul>
</li>
<li>응답 결과의 품질 저하를 피하고 길고 다양한 응답을 생성할 수 있도록 LoRA scaling factor의 강도를 낮춰서 적용</li>
</ul>
<p>Audio Clip을 기반으로 SALMONN으로 작성된 12개의 stories가 사용됨.
그 후 모델은 12단계동안 teacher-forcing-based-cross-entropy training으로 학습되며, 각 단계(step)에서는 오직 하나의 story sample만 사용되고 이는 SAMOLNN의 cross-modal의 emergent abilities를 활성화</p>
<ul>
<li>emergent abilities : 훈련되지 않았지만 모델이 방대한 양의 데이터를 학습해 새로운 특성을 가져 처음보는 데이터에 대해서도 좋은 결과를 내는 것</li>
</ul>
<h2 id="모델의-성능">모델의 성능<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#모델의-성능" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>단일 Task 특화 모델들이 처리하기 어려운 다양한 작업에서 좋은 성능을 보여줌</p>
<ul>
<li>음성 기반 slot filling</li>
<li>훈련되지 않은 언어의 음성 번역</li>
<li>오디오 기반 스토리텔링 생성</li>
<li>복합적 오디오 공동 추론</li>
</ul>
<h3 id="모델의-장점">모델의 장점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#모델의-장점" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>오디오 및 텍스트 데이터의 통합적 이해</li>
<li>훈련되지 않은 새롱누 작업에 적응 가능한 일반화 능력</li>
</ul>
<h3 id="모델의-단점">모델의 단점<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#모델의-단점" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ul>
<li>모델 훈련과 추론에 많은 컴퓨팅 자원이 필요
<ul>
<li>대규모 데이터 및 계산 능력</li>
</ul>
</li>
<li>복잡한 오디오 데이터 처리의 어려움
<ul>
<li>ex) 배경 소음이 있는 상황에서는 정확도가 떨어질 수 있음
(GPT 답이라 부정확할수도?)</li>
</ul>
</li>
</ul></article><hr/><div class="page-footer"><div class="giscus" data-repo="Jin-SukKim/Blog" data-repo-id="R_kgDOQHKGMA" data-category="General" data-category-id="DIC_kwDOQHKGMM4Cw975" data-mapping="url" data-strict="1" data-reactions-enabled="1" data-input-position="bottom" data-light-theme="light" data-dark-theme="dark" data-theme-url="https://jin-sukkim.github.io/Blog/static/giscus" data-lang="en"></div></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-7" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul id="list-1" class="toc-content overflow"><li class="depth-0"><a href="#모델의-목표" data-for="모델의-목표">모델의 목표</a></li><li class="depth-0"><a href="#모델의-구조" data-for="모델의-구조">모델의 구조</a></li><li class="depth-1"><a href="#window-level-q-former" data-for="window-level-q-former">Window-level Q-Former</a></li><li class="depth-1"><a href="#lora" data-for="lora">LoRA</a></li><li class="depth-0"><a href="#훈련-및-학습-방법" data-for="훈련-및-학습-방법">훈련 및 학습 방법</a></li><li class="depth-1"><a href="#학습-방법" data-for="학습-방법">학습 방법</a></li><li class="depth-0"><a href="#모델의-성능" data-for="모델의-성능">모델의 성능</a></li><li class="depth-1"><a href="#모델의-장점" data-for="모델의-장점">모델의 장점</a></li><li class="depth-1"><a href="#모델의-단점" data-for="모델의-단점">모델의 단점</a></li><li class="overflow-end"></li></ul></div><div class="recent-notes desktop-only"><h3>최근 포스트</h3><ul class="recent-ul"><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../../" class="internal">JEngine Blog</a></h3></div><p class="meta"><time datetime="2025-10-24T13:42:28.325Z">Oct 24, 2025</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../../content/" class="internal">JEngine Blog</a></h3></div><p class="meta"><time datetime="2025-10-24T13:42:28.323Z">Oct 24, 2025</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../../content/Study/AI/논문/SALMONN" class="internal">SALMONN</a></h3></div><p class="meta"><time datetime="2025-10-24T13:42:27.353Z">Oct 24, 2025</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../../content/Study/AI/논문/APT---Adaptive-Pruning-and-Tuning-Pretrained-Language-Models-forEfficient-Training-and-Inference" class="internal">APT - Adaptive Pruning and Tuning Pretrained Language Models forEfficient Training and Inference</a></h3></div><p class="meta"><time datetime="2025-10-24T13:42:27.346Z">Oct 24, 2025</time></p></div></li><li class="recent-li"><div class="section"><div class="desc"><h3><a href="../../../../content/Projects/Project-설정법/VS2022-프로젝트-설정" class="internal">VS2022 프로젝트 설정</a></h3></div><p class="meta"><time datetime="2025-10-24T13:42:27.177Z">Oct 24, 2025</time></p></div></li></ul></div></div><footer class><ul><li><a href="https://github.com/Jin-SukKim">GitHub</a></li></ul><p>© 2025 Jin-SukKim</p></footer></div></div></body><script type="application/javascript">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../../../postscript.js" type="module"></script></html>